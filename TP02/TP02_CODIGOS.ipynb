{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TABLA 1 ZOTENKO\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def ldata(archive):\n",
    "    f=open(archive)\n",
    "    data=[]\n",
    "    for line in f:\n",
    "        line=line.strip()\n",
    "        col=line.split()\n",
    "        data.append(col)\n",
    "    return data\n",
    "    \n",
    "#grafico de nodos\n",
    "def graficar(archive):\n",
    "\n",
    "    data=ldata(archive)\n",
    "    g = nx.Graph ()\n",
    "    g = g.to_undirected()\n",
    "    \n",
    "    for i in range (len(data)):\n",
    "        g.add_nodes_from (data[i])\n",
    "        g.add_edge (data[i][0],data[i][1])\n",
    "        \n",
    "    nx.draw(g,node_size = 40,with_labels=False, font_weight='bold')\n",
    "    plt.show()\n",
    "\n",
    "# graficar('tc02Data/yeast_AP-MS.txt')    \n",
    "# graficar('tc02Data/yeast_LIT.txt')\n",
    "# graficar('tc02Data/yeast_Y2H.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Tabla 1\n",
    "def data_de_red(archive):\n",
    "\n",
    "    RTA=[]\n",
    "    data=ldata(archive)\n",
    "    g = nx.Graph ()\n",
    "    g = g.to_undirected()\n",
    "\n",
    "    for i in range (len(data)):\n",
    "        g.add_nodes_from (data[i])\n",
    "        g.add_edge (data[i][0],data[i][1])\n",
    "\n",
    "    #Número de nodos\n",
    "    N=g.number_of_nodes()\n",
    "\n",
    "    #Número de enlaces\n",
    "    L=g.number_of_edges()\n",
    "\n",
    "    #Grado medio de la red\n",
    "    k=[]\n",
    "    for node in g.nodes:\n",
    "        k.append(g.degree(node)) #El comando g.degree() calcula el grado como \n",
    "                                #si la red fuese no-dirigida\n",
    "    km = sum(k)/len(k)  #valor medio de k  \n",
    "    \n",
    "    #Coeficiente de clustering\n",
    "\n",
    "    Nodo_gnodo_vecino_gvecino_vecinossegundos=[] #Creo una lista con los nodos,  \n",
    "    for nodo in g.nodes():                      #sus grados, sus vecinos, los \n",
    "        v=[]                                    #grados de sus vecinos y sus \n",
    "        for vecino in g.neighbors(nodo):        #vecinos segundos\n",
    "            v2=[]\n",
    "            for vecino2 in g.neighbors(vecino):\n",
    "                v2.append(vecino2)\n",
    "            gv=len(v2)\n",
    "            v.append([vecino,gv,v2])\n",
    "        gn=len(v)\n",
    "        Nodo_gnodo_vecino_gvecino_vecinossegundos.append([nodo,gn,v])\n",
    "\n",
    "    #Ahora calculo el <Ci>\n",
    "    t=-1\n",
    "    C=np.zeros(len(g.nodes()))\n",
    "\n",
    "    #Calculo la cantidad de enlaces entre vecinos\n",
    "    for n in Nodo_gnodo_vecino_gvecino_vecinossegundos:\n",
    "        E = 0\n",
    "        t=t+1\n",
    "        for i in range(len(n[2])):\n",
    "            for j in range(len(n[2])):\n",
    "                for k in range(len(n[2][i][2])):\n",
    "                    if n[2][i][2][k] == n[2][j][0]:\n",
    "                        E = E + 1 #Cuento los enlaces entre vecinos del nodo n. \n",
    "                                  #Haciendo esto, se cuentan dos veces.\n",
    "\n",
    "        if n[1] > 1: #Si el grado es menor o igual a 1, entonces el coeficiente \n",
    "                     #de clustering es nulo\n",
    "            C[t]=E/(n[1]*(n[1]-1)) #Como es una red no dirigida, debería \n",
    "                                   #multiplicar por dos, pero ese factor esta \n",
    "                                   #contemplado en el numero de enlaces \n",
    "                                   #calculados dado que se repiten dos veces\n",
    "\n",
    "    CPromedio=sum(C)/len(C)\n",
    "\n",
    "    RTA.append([N, L, km, CPromedio])\n",
    "\n",
    "    return RTA\n",
    "\n",
    "# AP = data_de_red('tc02Data/yeast_AP-MS.txt')\n",
    "# LIT = data_de_red('tc02Data/yeast_LIT.txt')\n",
    "# Y2H = data_de_red('tc02Data/yeast_Y2H.txt')\n",
    "# LIT_REGULY = data_de_red('tc02Data/yeast_LIT_Reguly.txt')\n",
    "\n",
    "# #TABLA\n",
    "# tabla_red = pd.DataFrame ({\"Redes\":[\"AP_MS\",\"LIT\",\"Y2H\",\"LIT_Reguly\"],\n",
    "#                            \"Numero de Nodos\":[AP[0][0],LIT[0][0],Y2H[0][0],LIT_REGULY[0][0]],\n",
    "#                            \"Numero de Enlaces\":[AP[0][1],LIT[0][1],Y2H[0][1],LIT_REGULY[0][1]],\n",
    "#                            \"Grado Medio\":[AP[0][2],LIT[0][2],Y2H[0][2],LIT_REGULY[0][2]],\n",
    "#                            \"<Ci>\":[AP[0][3],LIT[0][3],Y2H[0][3],LIT_REGULY[0][3]]})\n",
    "# print (tabla_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TABLA 2 ZOTENKO\n",
    "#Creo una funcion que agarre los enlaces de cada nodo y los compare con las otras redes\n",
    "LIT_REGULYY = ldata ('tc02Data/yeast_LIT_Reguly.txt')\n",
    "LIT_REGULYYY = []\n",
    "for i in range(1,len(LIT_REGULYY)):\n",
    "    LIT_REGULYYY.append([LIT_REGULYY[i][0],LIT_REGULYY[i][1]])\n",
    "#print(LIT_REGULYYY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Tabla2\n",
    "def invertir(lista):\n",
    "    rlista=[]\n",
    "    for i in range(len(lista)):\n",
    "        rlista.append(lista[len(lista)-1-i])\n",
    "    return rlista\n",
    "\n",
    "APP = ldata ('tc02Data/yeast_AP-MS.txt')\n",
    "LITT = ldata ('tc02Data/yeast_LIT.txt')\n",
    "Y2HH = ldata ('tc02Data/yeast_Y2H.txt')\n",
    "LIT_REGULYY = ldata ('tc02Data/yeast_LIT_Reguly.txt')\n",
    "\n",
    "LIT_REGULYYY = []\n",
    "for i in range(1,len(LIT_REGULYY)):\n",
    "    LIT_REGULYYY.append([LIT_REGULYY[i][0],LIT_REGULYY[i][1]])\n",
    "\n",
    "\n",
    "def overlap(data1,data2,data3,data4):\n",
    "    datas = [data1,data2,data3,data4]\n",
    "    overlap_list = []\n",
    "    for i in range(len(datas)):\n",
    "        i_list=[]\n",
    "        for j in range(i+1,len(datas)):\n",
    "            count = 0\n",
    "            for l in range (len(datas[i])):\n",
    "                if datas[i][l] in datas[j]:\n",
    "                    count = count + 1\n",
    "                else:\n",
    "                    invert = invertir(datas[i][l])\n",
    "                    if invert in datas[j]:\n",
    "                        count = count + 1\n",
    "            i_list.append(count)\n",
    "        overlap_list.append(i_list)\n",
    "    return overlap_list\n",
    "\n",
    "\n",
    "# overlap_list = overlap(APP,LITT,Y2HH,LIT_REGULYYY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           -         --        ---        ----\n",
      "0      AP_MS   0.443761  0.0887372    0.212497\n",
      "1   0.143109        LIT  0.0887372    0.241167\n",
      "2  0.0286659  0.0888889        Y2H   0.0403913\n",
      "3   0.277839   0.977778   0.163481  LIT_Reguly\n"
     ]
    }
   ],
   "source": [
    "#TABLA\n",
    "tabla_red = pd.DataFrame ({\"-\":[\"AP_MS\",overlap_list[0][0]/AP[0][1],overlap_list[0][1]/AP[0][1],overlap_list[0][2]/AP[0][1]],\n",
    "                           \"--\":[overlap_list[0][0]/LIT[0][1],\"LIT\",overlap_list[1][0]/LIT[0][1],overlap_list[1][1]/LIT[0][1]],\n",
    "                           \"---\":[overlap_list[0][1]/Y2H[0][1],overlap_list[1][0]/Y2H[0][1],\"Y2H\",overlap_list[2][0]/Y2H[0][1]],\n",
    "                           \"----\":[overlap_list[0][2]/LIT_REGULY[0][1],overlap_list[1][1]/LIT_REGULY[0][1],overlap_list[2][0]/LIT_REGULY[0][1],\"LIT_Reguly\"]})\n",
    "print (tabla_red)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Grafico 3\n",
    "def asignar_esencialidad(data):\n",
    "    escen = ldata ('tc02Data/Essential_ORFs_paperHe.txt')\n",
    "    #data=ldata(archive)\n",
    "    g = nx.Graph ()\n",
    "    g = g.to_undirected()\n",
    "\n",
    "    for i in range (len(data)):\n",
    "        g.add_nodes_from (data[i])\n",
    "        g.add_edge (data[i][0],data[i][1])\n",
    "        \n",
    "    N=g.number_of_nodes()\n",
    "    \n",
    "    k=[]\n",
    "    for node in g.nodes:\n",
    "        k.append(g.degree(node))\n",
    "    \n",
    "    kM = max(k)\n",
    "    \n",
    "    nodos=[]\n",
    "          \n",
    "    for node in g.nodes():\n",
    "        g.node[node]['escencialidad']='no escencial'\n",
    "        nodos.append([node,'no escencial'])\n",
    "        h=nodos.index([node,'no escencial'])\n",
    "        for j in range(len(escen)):\n",
    "\n",
    "            if len(escen[j]) > 2 and node in escen[j][1]: #No me interesan las listas con largo menor a 2\n",
    "                g.node[node]['escencialidad']='escencial'\n",
    "                nodos[h][1]='escencial'\n",
    "        \n",
    "    frac_Hubs_total = []\n",
    "    frac_Escen_Hubs = []\n",
    "    for k in range(1,kM):#defino el kcutoff\n",
    "        contadorH = 0\n",
    "        contadorE = 0\n",
    "        for node in nodos:\n",
    "            #K_cutoff.append(0)\n",
    "            if g.degree(node[0]) > k:\n",
    "                contadorH = contadorH + 1\n",
    "                if node[1] == 'escencial':\n",
    "                    contadorE = contadorE + 1\n",
    "                    \n",
    "        frac_Hubs_total.append(contadorH/N)\n",
    "        \n",
    "        frac_Escen_Hubs.append(contadorE/contadorH)\n",
    "\n",
    "    #print(frac_Hubs_total)\n",
    "    #print(frac_Escen_Hubs)\n",
    "    plt.plot(frac_Hubs_total,frac_Escen_Hubs)\n",
    "    plt.xlabel('Fraccion de Hubs sobre total de nodos')\n",
    "    plt.ylabel('Fraccion de Nodos escenciales sobre total de Hubs')\n",
    "\n",
    "\n",
    "# asignar_esencialidad(APP)\n",
    "# asignar_esencialidad(LITT)\n",
    "# asignar_esencialidad(Y2HH)\n",
    "# asignar_esencialidad(LIT_REGULYYY)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#dataAPP = asignar_esencialidad(APP)\n",
    "#dataLIT = asignar_esencialidad(LITT)\n",
    "#dataY2HH = asignar_esencialidad(Y2HH)\n",
    "#dataLITREG = asignar_esencialidad(LIT_REGULYYY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analisis de vulnerabilidad\n",
    "#Figura 3 Zotenko.\n",
    "\n",
    "#Armo el grafo de cada set de datos.\n",
    "def grafo (archive):\n",
    "    data=ldata(archive)\n",
    "    g = nx.Graph ()\n",
    "    g = g.to_undirected()\n",
    "\n",
    "    for i in range (len(data)):\n",
    "        g.add_nodes_from (data[i])\n",
    "        g.add_edge (data[i][0],data[i][1])\n",
    "    return g\n",
    "\n",
    "#La V es de vulnerabilidad.\n",
    "V_AP = grafo ('tc02Data/yeast_AP-MS.txt')\n",
    "V_LIT = grafo ('tc02Data/yeast_LIT.txt')\n",
    "V_Y2H = grafo ('tc02Data/yeast_Y2H.txt')\n",
    "V_LIT_REGULY = grafo ('tc02Data/yeast_LIT_Reguly.txt')\n",
    "\n",
    "#Extraigo la información del grado de cada nodo. En dos listas separadas para identificar al nodo con grado mas alto.\n",
    "nodos_V_AP = []\n",
    "grados_V_AP = []\n",
    "for node in V_AP.nodes:\n",
    "    nodos_V_AP.append (node)\n",
    "    grados_V_AP.append (V_AP.degree(node))\n",
    "\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++    \n",
    "#Remuevo grado\n",
    "#Modificando el valor central analizado, donde dice comp_gig.degree se pueden analizar eigenvalue y shortest-path.\n",
    "def impacto_grado (archive):\n",
    "    x = []\n",
    "    y = []\n",
    "    #Tomo la componente gigante de mi grafo. Como criterio se toma el largo (key = len) y tomo el subgrafo con el máximo.\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    N = int (comp_gig.number_of_nodes())\n",
    "    i = 0\n",
    "    while len (comp_gig) > 400: #Modificando el numero podemos determinar hasta cuando va a analizar el largo de la red.\n",
    "        nodos = [] #Nodos de la componente gigante.\n",
    "        grados = [] #Grado de ellos.\n",
    "        for node in  comp_gig.nodes:\n",
    "            nodos.append (node)\n",
    "            grados.append (comp_gig.degree(node))\n",
    "        indice_nodo_grado_mayor = grados.index(max(grados))\n",
    "        comp_gig.remove_node(nodos[indice_nodo_grado_mayor])\n",
    "        i = i + 1\n",
    "        x.append (i/float (N))\n",
    "        comp_gig = max (nx.connected_component_subgraphs(comp_gig) , key = len)\n",
    "        y.append(len(comp_gig)/float (N)) #IMPORTANTE:El numerador es el largo NUEVO de la componente gigante y\n",
    "                                          #el denominador te da el largo antes de la extracción del nodo central.\n",
    "        \n",
    "    todo = [x,y]\n",
    "    return todo\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#REMOVE RANDOM\n",
    "def impacto_random (archive):\n",
    "    x = []\n",
    "    y = []\n",
    "    #Tomo la componente gigante de mi grafo. Como criterio se toma el largo (key = len) y tomo el subgrafo con el máximo.\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    N = int (comp_gig.number_of_nodes())\n",
    "    i = 0\n",
    "    while len (comp_gig) > 400: #Modificando el numero podemos determinar hasta cuando va a analizar el largo de la red.\n",
    "        nodos = [] #Nodos de la componente gigante.\n",
    "        grados = [] #Grado de ellos.\n",
    "        for node in  comp_gig.nodes:\n",
    "            nodos.append (node)\n",
    "            grados.append (comp_gig.degree(node))\n",
    "        nodo = random.choice(nodos) #elijo nodo aleatoriamente\n",
    "        comp_gig.remove_nodes_from([nodo])\n",
    "        i = i + 1\n",
    "        x.append (i/float (N))\n",
    "        comp_gig = max (nx.connected_component_subgraphs(comp_gig) , key = len)\n",
    "        y.append(len(comp_gig)/float (N)) #IMPORTANTE:El numerador es el largo NUEVO de la componente gigante y\n",
    "                                          #el denominador te da el largo antes de la extracción del nodo central.\n",
    "        \n",
    "    todo = [x,y]\n",
    "    return todo\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#REMOVE ESSENTIAL\n",
    "def impacto_escencial (archive):\n",
    "#--------------------------------------asigno escencialidad a los nodos-------------------------------------------------\n",
    "    escen = ldata ('tc02Data/Essential_ORFs_paperHe.txt') \n",
    "    Nodos_de_red_original = []\n",
    "    Nodos_escenciales_original = []     \n",
    "    for node in archive.nodes():\n",
    "        archive.node[node]['escencialidad']='no escencial'\n",
    "        Nodos_de_red_original.append([node,'no escencial'])\n",
    "        h=Nodos_de_red_original.index([node,'no escencial'])\n",
    "        for j in range(len(escen)):\n",
    "            \n",
    "            if len(escen[j]) > 2 and node in escen[j][1]: #No me interesan las listas con largo menor a 2\n",
    "                archive.node[node]['escencialidad']='escencial'\n",
    "                Nodos_de_red_original[h][1]='escencial'\n",
    "                Nodos_escenciales_original.append(node)\n",
    "                \n",
    "#Creo una lista de nodos escenciales\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "    x = []\n",
    "    y = []\n",
    "    #Tomo la componente gigante de mi grafo. Como criterio se toma el largo (key = len) y tomo el subgrafo con el máximo.\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    N = int (comp_gig.number_of_nodes())\n",
    "    i = 0\n",
    "    nodos_escenciales = Nodos_escenciales_original\n",
    "    while len (nodos_escenciales) > 1: #Modificando el numero podemos determinar hasta cuando va a analizar el largo de la red.\n",
    "        nodos = [] #Nodos de la componente gigante.\n",
    "        grados = [] #Grado de ellos.\n",
    "        nodos_escenciales = []\n",
    "        for node in  comp_gig.nodes:\n",
    "            nodos.append (node)\n",
    "            grados.append (comp_gig.degree(node))\n",
    "            if node in Nodos_escenciales_original:\n",
    "                nodos_escenciales.append(node)\n",
    "        nodo = random.choice(nodos_escenciales) #elijo nodo escencial aleatoriamente\n",
    "        comp_gig.remove_nodes_from([nodo])\n",
    "        i = i + 1\n",
    "        x.append (i/float (N))\n",
    "        comp_gig = max (nx.connected_component_subgraphs(comp_gig) , key = len)\n",
    "        y.append(len(comp_gig)/float (N)) #IMPORTANTE:El numerador es el largo NUEVO de la componente gigante y\n",
    "                                              #el denominador te da el largo antes de la extracción del nodo central.\n",
    "        \n",
    "    todo = [x,y]\n",
    "    return todo\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#Impacto eigenvector\n",
    "def impacto_eigenvector (archive):\n",
    "    x = []\n",
    "    y = []\n",
    "    #Tomo la componente gigante de mi grafo. Como criterio se toma el largo (key = len) y tomo el subgrafo con el máximo.\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    N = int (comp_gig.number_of_nodes())\n",
    "    i = 0\n",
    "    while len (comp_gig) > 400: #Modificando el numero podemos determinar hasta cuando va a analizar el largo de la red.\n",
    "        nodos = [] #Nodos de la componente gigante.\n",
    "        eigenvector = [] #Grado de ellos.\n",
    "        for node in  comp_gig.nodes:\n",
    "            nodos.append (node)\n",
    "            eigenvector.append (nx.eigenvector_centrality(comp_gig)[node])\n",
    "        indice_nodo_autovector_mayor = eigenvector.index(max(grados))\n",
    "        comp_gig.remove_node(nodos[indice_nodo_autovector_mayor])\n",
    "        i = i + 1\n",
    "        x.append (i/float (N))\n",
    "        comp_gig = max (nx.connected_component_subgraphs(comp_gig) , key = len)\n",
    "        y.append(len(comp_gig)/float (N)) #IMPORTANTE:El numerador es el largo NUEVO de la componente gigante y\n",
    "                                          #el denominador te da el largo antes de la extracción del nodo central.\n",
    "        \n",
    "    todo = [x,y]\n",
    "    return todo\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "#Ver el impacto del betweenness.\n",
    "def impacto_bet (archive):\n",
    "    x = []\n",
    "    y = []\n",
    "    #Tomo la componente gigante de mi grafo. Como criterio se toma el largo (key = len) y tomo el subgrafo con el máximo.\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    N = int (comp_gig.number_of_nodes())\n",
    "    i = 0\n",
    "    #print (nx.betweenness_centrality(comp_gig)[\"YNL301C\"])\n",
    "    print (len (comp_gig))\n",
    "    while len (comp_gig) > 970: #Modificando el numero podemos determinar hasta cuando va a analizar el largo de la red.\n",
    "        nodos = [] #Nodos de la componente gigante.\n",
    "        bet = [] #Grado de ellos.\n",
    "        for node in  comp_gig.nodes:\n",
    "            nodos.append (node)\n",
    "            #print (nodos)\n",
    "            #print (bet)\n",
    "            bet.append (nx.betweenness_centrality(comp_gig)[node])        \n",
    "        indice_nodo_bet_mayor = bet.index(max(bet))\n",
    "        comp_gig.remove_node(nodos[indice_nodo_bet_mayor])\n",
    "        i = i + 1\n",
    "        x.append (i/float (N))\n",
    "        comp_gig = max (nx.connected_component_subgraphs(comp_gig) , key = len)\n",
    "        y.append(len(comp_gig)/float (N)) #IMPORTANTE:El numerador es el largo NUEVO de la componente gigante y\n",
    "                                          #el denominador te da el largo antes de la extracción del nodo central.\n",
    "        \n",
    "    todo = [x,y]\n",
    "    return todo\n",
    "#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "# pruebagrado = impacto_grado (V_AP)\n",
    "\n",
    "# plt.plot(pruebagrado[0],pruebagrado[1],'r')\n",
    "# plt.xlabel('Fraccion de nodos extraidos')\n",
    "# plt.ylabel('Fraccion de nodos que quedan')\n",
    "\n",
    "\n",
    "# pruebaescencial = impacto_escencial (V_AP)\n",
    "\n",
    "# plt.plot(pruebaescencial[0],pruebaescencial[1],'b')\n",
    "# plt.xlabel('Fraccion de nodos extraidos')\n",
    "# plt.ylabel('Fraccion de nodos que quedan')\n",
    "\n",
    "\n",
    "# pruebagrado = impacto_random (V_AP)\n",
    "\n",
    "# plt.plot(pruebagrado[0],pruebagrado[1],'k')\n",
    "# plt.xlabel('Fraccion de nodos extraidos')\n",
    "# plt.ylabel('Fraccion de nodos que quedan')\n",
    "\n",
    "# pruebavector = impacto_eigenvector (V_AP)\n",
    "\n",
    "# plt.plot(pruebavector[0],pruebavector[1],'k')\n",
    "# plt.xlabel('Fraccion de nodos extraidos')\n",
    "# plt.ylabel('Fraccion de nodos que quedan')\n",
    "\n",
    "# plt.show ()\n",
    "\n",
    "\n",
    "#Creeria que esta bien, el grafico no me gusta mucho igual.\n",
    "\n",
    "#Para nodos aleatoriamente esta en la linea de arriba (32).\n",
    "\n",
    "\n",
    "#gonzauri@gmail.com\n",
    "\n",
    "#Para la extraccion de proteinas escenciales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#FIGURA 2.B HE\n",
    "#CALCULO ALFA\n",
    "#Para eso, tengo que calcular la distribucion del numero de interacciones entre proteinas escenciales (IBEP) \n",
    "# en redes recableadas aleatoriamente. Siendo n el nro de interacciones escenciales (essential PPI) y N el nro\n",
    "#total de enlaces en la red, alfa=(n-m)/N\n",
    "\n",
    "#Agarro la red y la recableo 10000 veces , calculando m en cada recableado\n",
    "def recableado_random(g):\n",
    "    \n",
    "    nodos=[]\n",
    "    #Asigno escencialidad\n",
    "    for node in g.nodes():\n",
    "        g.node[node]['escencialidad']='no escencial'\n",
    "        nodos.append([node,'no escencial'])\n",
    "        h=nodos.index([node,'no escencial'])\n",
    "        for j in range(len(escen)):\n",
    "\n",
    "            if len(escen[j]) > 2 and node in escen[j][1]: #No me interesan las listas con largo menor a 2\n",
    "                g.node[node]['escencialidad']='escencial'\n",
    "                nodos[h][1]='escencial'\n",
    "    \n",
    "    #Calculo el m de la red original\n",
    "    lista_de_m = []\n",
    "    m = 0\n",
    "    for enlace in nuevos_edges:\n",
    "        if g.node[enlace[0]]['escencialiad'] == 'escencial' and g.node[enlace[1]]['escencialiad'] == 'escencial':\n",
    "            m = m + 1\n",
    "    lista_de_m.append(m)\n",
    "\n",
    "    # nx.draw(g,node_size = 40,with_labels=True, font_weight='bold')\n",
    "    # plt.show()\n",
    "    #Lista de nodos con grados ordenada de mayor a menor sin nodos con grado cero\n",
    "    nodo_grados=[]\n",
    "    for node in g.nodes:\n",
    "        nodo_grados.append([node,g.degree[node]])\n",
    "        if g.degree(node) == 0:\n",
    "            nodo_grados.remove([node,g.degree[node]])\n",
    "    nodo_grados.sort(key=lambda grado: grado[1], reverse=True)\n",
    "\n",
    "    #Arranca el recableado\n",
    "\n",
    "    #PASOS A SEGUIR\n",
    "    #I)   Tengo una lista con los nodos y sus grados\n",
    "    #II)  agarro el nodo de grado mas alto\n",
    "    #III) lo voy recableando y redefiniendo grados de sus nodos vecinos (para cada nuevo vecino conectado a este nodo, su grado se reduce en 1)\n",
    "    #IV)  una vez recableado, rehago la lista de nodos y grados, eliminando a este nodo de la lista, y con los grados del resto redefinidos,\n",
    "    #     y repito\n",
    "\n",
    "    for _ in range(10):\n",
    "        nuevos_edges=[]\n",
    "        while len(nodo_grados)>1: #(II)\n",
    "            while nodo_grados[0][1] > 0:\n",
    "                nuevo_vecino = random.choice(nodo_grados[1:])\n",
    "                if [nodo_grados[0][0],nuevo_vecino[0]] not in nuevos_edges: #(III)\n",
    "                    nuevo_vecino[1] = nuevo_vecino[1] - 1\n",
    "                    nodo_grados[0][1] = nodo_grados[0][1] -1\n",
    "                    nuevos_edges.append([nodo_grados[0][0],nuevo_vecino[0]]) #mi nueva lista de enlaces puede tener menos enlaces que la original, pero no sera significativo\n",
    "                    if nuevo_vecino[1] == 0:\n",
    "                        nodo_grados.remove(nuevo_vecino)\n",
    "            nodo_grados.remove(nodo_grados[0]) #(IV)\n",
    "            nodo_grados.sort(key=lambda grado: grado[1], reverse=True) #Reordeno la lista con los nodos redefinidos y con los nodos con k = 0 fuera\n",
    "\n",
    "        #me fijo cuantos enlaces escenciales tengo\n",
    "        m = 0\n",
    "        for enlace in nuevos_edges:\n",
    "            if g.node[enlace[0]]['escencialiad'] == 'escencial' and g.node[enlace[1]]['escencialiad'] == 'escencial':\n",
    "                m = m + 1\n",
    "        lista_de_m.append(m)\n",
    "\n",
    "#CREO QUE TENGO QUE LEER MEJOR LA PARTE QUE HABLA DE LOS IBEP Y PPI ESCENCIALES PORQUE NO ESTARIA ENTENDIENDO\n",
    "#COMO UN IBEP NO ES PPI ESCENCIAL\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[['gonza','pablo'],['yoda','obi'],['pancho','obi'],['carlitos','pablo'],['gonza','pancho'],['gonza','carlitos'],['obi','carlitos'],['obi','braulio']]\n",
    "g = nx.Graph ()\n",
    "g = g.to_undirected()\n",
    "\n",
    "for i in range (len(data)):\n",
    "    g.add_nodes_from (data[i])\n",
    "    g.add_edge (data[i][0],data[i][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TABLA 3 DE ZOTENKO\n",
    "#PASOS\n",
    "#creo una lista con los nodos escenciales de la componente gigante\n",
    "#creo una lista con los grados de los nodos escenciales\n",
    "#remuevo los nodos escenciales de la componente gigante y rearmo el grafo con los nodos restantes\n",
    "#analizo la nueva componente gigante\n",
    "#\n",
    "#elijo de forma aleatoria nodos no escenciales tales que tenga una misma cantidad que escenciales y misma distribucion de grado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### el autovalor de mi matriz de adyacencia tiene asociado un autovector. Las n posiciones de mi autovector me dan la centralidad de cada uno de esos nodos en la red. Entonces, lo que hago es buscar el autovalor mas alto y busco el nodo correspondiente (dentro del autovector asociado) de centralidad mas alta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impacto eigenvector\n",
    "def impacto_eigenvector (archive):\n",
    "    x = []\n",
    "    y = []\n",
    "    #Tomo la componente gigante de mi grafo. Como criterio se toma el largo (key = len) y tomo el subgrafo con el máximo.\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    N = int (comp_gig.number_of_nodes())\n",
    "    i = 0\n",
    "    while len (comp_gig) > 400: #Modificando el numero podemos determinar hasta cuando va a analizar el largo de la red.\n",
    "        nodos = [] #Nodos de la componente gigante.\n",
    "        eigenvector = [] #Grado de ellos.\n",
    "        for node in  comp_gig.nodes:\n",
    "            nodos.append (node)\n",
    "            eigenvector.append (nx.eigenvector_centrality(comp_gig)[node])\n",
    "        indice_nodo_autovector_mayor = eigenvector.index(max(eigenvector))\n",
    "        comp_gig.remove_node(nodos[indice_nodo_autovector_mayor])\n",
    "        i = i + 1\n",
    "        x.append (i/float (N))\n",
    "        comp_gig = max (nx.connected_component_subgraphs(comp_gig) , key = len)\n",
    "        y.append(len(comp_gig)/float (N)) #IMPORTANTE:El numerador es el largo NUEVO de la componente gigante y\n",
    "                                          #el denominador te da el largo antes de la extracción del nodo central.\n",
    "        \n",
    "    todo = [x,y]\n",
    "    return todo\n",
    "#+++++++++++++++++\n",
    "# pruebavector = impacto_eigenvector (V_AP)\n",
    "\n",
    "# plt.plot(pruebavector[0],pruebavector[1],'k')\n",
    "# plt.xlabel('Fraccion de nodos extraidos')\n",
    "# plt.ylabel('Fraccion de nodos que quedan')\n",
    "\n",
    "# plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[630, 1622, 630, 630, 977]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[344, 0.32270916334661354, 0.5388446215139442]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TABLA 3 ZOTENKO\n",
    "#REMOVE ESSENTIAL\n",
    "def impacto_escencial_total (archive):\n",
    "#--------------------------------------asigno escencialidad a los nodos-------------------------------------------------\n",
    "    escen = ldata ('tc02Data/Essential_ORFs_paperHe.txt') \n",
    "    Nodos_de_red_original = []\n",
    "    Nodos_escenciales_y_grado = []     \n",
    "    for node in archive.nodes():\n",
    "        archive.node[node]['escencialidad']='no escencial'\n",
    "        Nodos_de_red_original.append([node,'no escencial'])\n",
    "        h=Nodos_de_red_original.index([node,'no escencial'])\n",
    "        for j in range(len(escen)):\n",
    "\n",
    "            if len(escen[j]) > 2 and node in escen[j][1]: #No me interesan las listas con largo menor a 2\n",
    "                archive.node[node]['escencialidad']='escencial'\n",
    "                Nodos_de_red_original[h][1]='escencial'\n",
    "                Nodos_escenciales_y_grado.append([node,archive.degree(node)])#Lista con grados de cada nodo escencial\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    Ninicial = int (comp_gig.number_of_nodes())\n",
    "    nodos_escenciales_en_comp_gig = 0\n",
    "    for node in Nodos_escenciales_y_grado:\n",
    "        if node[0] in comp_gig.nodes:\n",
    "            comp_gig.remove_node(node[0])\n",
    "            nodos_escenciales_en_comp_gig = nodos_escenciales_en_comp_gig + 1\n",
    "            comp_gig = max (nx.connected_component_subgraphs(comp_gig) , key = len)\n",
    "\n",
    "    Nfinal = int (comp_gig.number_of_nodes())\n",
    "    x = nodos_escenciales_en_comp_gig\n",
    "    y = float (Nfinal)/float (Ninicial)\n",
    "    \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "    comp_gig_2 = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    nodos_escenciales = []\n",
    "    grados_escenciales = []\n",
    "    for i in range (len(Nodos_escenciales_y_grado)):\n",
    "        nodos_escenciales.append (Nodos_escenciales_y_grado[i][0])\n",
    "        grados_escenciales.append (Nodos_escenciales_y_grado[i][1])\n",
    "    #Primero armo una lista con todos los nodos no escenciales con el mismo grado que los escenciales.\n",
    "    #Se puede mejorar calculando un promedio del grado de los nodos escenciales y eligiendo nodos con ese grado o cercano.\n",
    "    nodos_no_escenciales_mismo_grado = []\n",
    "    for node in Nodos_de_red_original:\n",
    "        if archive.degree(node[0]) in grados_escenciales and not node[0] in nodos_escenciales:\n",
    "            nodos_no_escenciales_mismo_grado.append (node[0])\n",
    "            \n",
    "    #Ahora elijo aleatoriamente nodos de esta ultima lista (nodos_no...)\n",
    "    j = 0\n",
    "    while j <= x:\n",
    "        nodo_aleatorio = random.choice(nodos_no_escenciales_mismo_grado)\n",
    "        if nodo_aleatorio in comp_gig_2.nodes:\n",
    "            comp_gig_2.remove_node(nodo_aleatorio)\n",
    "            comp_gig_2 = max (nx.connected_component_subgraphs(comp_gig_2) , key = len)\n",
    "            j = j + 1\n",
    "    Nfinal_2 = int (comp_gig_2.number_of_nodes())\n",
    "    y_2 = float (Nfinal_2) / float (Ninicial)\n",
    "    #Estas dos lineas solo sirven para tener referencias de que las listas construidas tienen largos razonables.\n",
    "    lista = [len(nodos_escenciales), len (Nodos_de_red_original), len (Nodos_escenciales_y_grado), len (grados_escenciales), len (nodos_no_escenciales_mismo_grado)]    \n",
    "    print (lista)\n",
    "    #--------------\n",
    "    todo = [x,y,y_2]\n",
    "    return todo\n",
    "\n",
    "prueba = impacto_escencial_total(V_AP)\n",
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TABLA 3 ZOTENKO\n",
    "#REMOVE ESSENTIAL\n",
    "def impacto_escencial_total (archive):\n",
    "#--------------------------------------asigno escencialidad a los nodos-------------------------------------------------\n",
    "    escen = ldata ('tc02Data/Essential_ORFs_paperHe.txt') \n",
    "    Nodos_de_red_original = []\n",
    "    Nodos_escenciales_y_grado = []     \n",
    "    for node in archive.nodes():\n",
    "        archive.node[node]['escencialidad']='no escencial'\n",
    "        Nodos_de_red_original.append([node,'no escencial'])\n",
    "        h=Nodos_de_red_original.index([node,'no escencial'])\n",
    "        for j in range(len(escen)):\n",
    "            \n",
    "            if len(escen[j]) > 2 and node in escen[j][1]: #No me interesan las listas con largo menor a 2\n",
    "                archive.node[node]['escencialidad']='escencial'\n",
    "                Nodos_de_red_original[h][1]='escencial'\n",
    "                Nodos_escenciales_y_grado.append([node,archive.degree(node)])#Lista con grados de cada nodo escencial\n",
    "                \n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "    comp_gig = max (nx.connected_component_subgraphs(archive) , key = len)\n",
    "    Ninicial = int (comp_gig.number_of_nodes())\n",
    "    nodos_escenciales_en_comp_gig = 0\n",
    "    lista=list(comp_gig.nodes)\n",
    "    return lista\n",
    "\n",
    "prueba = impacto_escencial_total(V_AP)\n",
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
